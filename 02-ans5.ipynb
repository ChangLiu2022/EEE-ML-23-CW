{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# k-fold Validation\n",
        "\n",
        "\n",
        "\n",
        "In machine learning, assessing the performance of a model is crucial, especially when working with limited data. One powerful technique for evaluating models is k-Fold Cross-Validation. This method involves partitioning the dataset into multiple subsets, or \"folds,\" to comprehensively assess a model's generalization capability.\n",
        "\n",
        "The term \"k\" in k-Fold refers to the number of folds the dataset is divided into. Typically, a common choice is k=10, resulting in 10-fold cross-validation. The primary goal of this technique is to estimate how well a machine learning model will perform on unseen data. By simulating the model's performance across multiple partitions of the dataset, we gain insights into its general behavior and potential for overfitting.\n",
        "\n",
        "\n",
        "The procedure for k-Fold Cross-Validation is as follows:\n",
        "\n",
        "1. Randomly shuffle the dataset to ensure randomness in fold assignments.\n",
        "2. Divide the dataset into \"k\" approximately equal-sized groups (folds).\n",
        "3. For each fold \"i\" (from 1 to k):\n",
        "   - Treat fold \"i\" as the validation set.\n",
        "   - Use the remaining k-1 folds as the training set.\n",
        "   - Train a machine learning model on the training set and evaluate it on the validation set.\n",
        "   - Retain the evaluation score and discard the model.\n",
        "4. Calculate the mean and optionally the variance (e.g., standard deviation) of the evaluation scores to summarize the model's performance.\n",
        "\n",
        "It's crucial to note that each data point appears in the validation set once and in the training set k-1 times, ensuring comprehensive assessment.\n",
        "\n",
        "\n",
        "1. **Less Bias**: k-Fold Cross-Validation often yields less biased estimates of model performance compared to a simple train/test split.\n",
        "2. **Comprehensive Evaluation**: All data points get an opportunity to be both in training and validation sets, leading to a more holistic evaluation.\n",
        "3. **Hyperparameter Tuning**: Hyperparameter tuning can be done on the training set within each fold, preventing data leakage and over-optimistic estimates.\n",
        "4. **Variance Estimate**: By calculating the variance of evaluation scores, we gain insights into the model's stability across different subsets of data.\n",
        "\n",
        "## Task 5 [25 marks]\n",
        "\n",
        "In this task, you will implement k-Fold Cross-Validation on the Diabetes dataset using scikit-learn.\n",
        "\n",
        "Your task is to write a code that performs the following steps:\n",
        "\n",
        "1. Load the Diabetes dataset.\n",
        "2. Define the number of folds for cross-validation.\n",
        "3. Calculate the number of samples per fold.\n",
        "4. Perform k-Fold Cross-Validation:\n",
        "   - For each fold:\n",
        "     - Split the data into training and test sets.\n",
        "     - Train a Linear Regression model on the training set.\n",
        "     - Make predictions on the test set.\n",
        "     - Calculate the Mean Squared Error (MSE) for the predictions.\n",
        "     - Print the MSE for each fold.\n",
        "     - Store the MSE values in a list.\n",
        "5. Calculate the average MSE across all folds and print it.\n",
        "\n",
        "> In this task you are allowed to use scikit-learn library to fit the linear regression model and for calculating the MSE. However, you are expected to create your own code (using numpy) for k-fold validation set. Below are the corresponding commands that you can import:\n",
        "- `from sklearn.linear_model import LinearRegression`\n",
        "- `from sklearn.metrics import mean_squared_error`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2512.921130288053\n",
            "1356.78908576085\n",
            "3673.3256000439596\n",
            "2197.12267605005\n",
            "4203.846365617799\n",
            "3117.53711285309\n",
            "1610.6201354834727\n",
            "5168.873084464219\n",
            "3705.1646356286537\n",
            "2264.699423407425\n",
            "2921.950381901836\n",
            "3140.6488083813724\n",
            "2612.732363885544\n",
            "avg mse\n",
            "2960.47929259741\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Load the diabetes dataset\n",
        "diabetes = load_diabetes()\n",
        "\n",
        "# Create a DataFrame to store the data and target\n",
        "diabetes_data = pd.DataFrame(data=diabetes['data'], columns=diabetes['feature_names'])\n",
        "diabetes_data[\"DP\"] = diabetes['target']\n",
        "\n",
        "x = diabetes_data.iloc[:, 0:9].to_numpy()\n",
        "y = diabetes_data.iloc[:, 10].to_numpy()\n",
        "\n",
        "n_folds = 13\n",
        "if(len(diabetes_data[\"DP\"])%n_folds != 0):\n",
        "    raise Exception('datasize is not divisible by n_folds')\n",
        "else:\n",
        "    n_samples = len(diabetes_data[\"DP\"])/n_folds\n",
        "\n",
        "mse_all = []\n",
        "for fold_i in range(n_folds):\n",
        "    train_x = np.vstack((x[:fold_i*n_folds,:],x[(fold_i+1)*n_folds:,:]))\n",
        "    val_x = x[fold_i*n_folds:(fold_i+1)*n_folds,:]\n",
        "    train_y = np.hstack((y[:fold_i*n_folds],y[(fold_i+1)*n_folds:]))\n",
        "    val_y = y[fold_i*n_folds:(fold_i+1)*n_folds]\n",
        "    reg = LinearRegression().fit(train_x, train_y)\n",
        "    print(mean_squared_error(val_y, reg.predict(val_x)))\n",
        "    mse_all.append(mean_squared_error(val_y, reg.predict(val_x)))\n",
        "\n",
        "avg_mse = np.average(np.array(mse_all))\n",
        "print(\"avg mse\")\n",
        "print(avg_mse)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOcC5w2fWZrSfNY0dvgwhZS",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
